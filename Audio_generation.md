# ğŸ§ The Frequency Quest  
### *Conditional WGAN-GP Audio Generation with HiFi-GAN Vocoder*  

![Python](https://img.shields.io/badge/Python-3.10-blue)  
![PyTorch](https://img.shields.io/badge/PyTorch-Framework-orange)  
![License](https://img.shields.io/badge/License-MIT-green)

---

## ğŸ§  Overview

**The Frequency Quest** is a deep learning project that combines **Conditional Generative Adversarial Networks (CGAN)** with **Wasserstein loss and Gradient Penalty (WGAN-GP)** to generate **Mel-spectrograms** conditioned on class labels.  
The generated Mel-spectrograms are then converted into audible sound using the **HiFi-GAN Vocoder** for realistic, high-quality audio synthesis.  

This project is ideal for learning, research, and experimentation in **neural audio generation**, **GAN stability**, and **conditional generative modeling**.

---

## âš™ï¸ Key Features

- ğŸ›ï¸ **WGAN-GP Framework** â€” Stable GAN training with gradient penalty for better convergence  
- ğŸšï¸ **Conditional Generation** â€” Class-based audio generation using label embeddings  
- ğŸ§© **HiFi-GAN Integration** â€” Converts Mel-spectrograms into high-fidelity `.wav` audio  
- ğŸ—ƒï¸ **Custom Dataset Loader** â€” Converts `.wav` to Mel-spectrograms and pads/trims automatically  
- ğŸ“Š **Training Visualization** â€” Saves loss curves, generated samples, and spectrograms after each epoch  

---

## ğŸ§© Architecture Overview

### ğŸ¨ **1ï¸âƒ£ Generator**
- Input â†’ latent noise vector `z` + one-hot encoded class label  
- Output â†’ 80Ã—512 Mel-spectrogram  
- Layers â†’ Dense + ConvTranspose2D + ReLU  
- Output activation â†’ ReLU (to match `log1p` Mel scale)

### ğŸ§± **2ï¸âƒ£ Discriminator**
- Input â†’ Mel-spectrogram + label embedding  
- Output â†’ Wasserstein critic score (realness measure)  
- Layers â†’ Conv2D + LeakyReLU  
- No BatchNorm (as per WGAN-GP design)

### ğŸ”Š **3ï¸âƒ£ HiFi-GAN Vocoder**
- Converts generated Mel-spectrograms into `.wav` audio  
- Loaded via `torchaudio.prototype.pipelines` pretrained HiFi-GAN  

---

## ğŸ“¦ Dataset Preparation

Your dataset must be structured like this:

```
dataset/
 â”œâ”€â”€ train/
 â”‚    â”œâ”€â”€ class_1/
 â”‚    â”‚     â”œâ”€â”€ file1.wav
 â”‚    â”‚     â”œâ”€â”€ file2.wav
 â”‚    â”œâ”€â”€ class_2/
 â”‚    â”‚     â”œâ”€â”€ file1.wav
 â”‚    â”‚     â”œâ”€â”€ file2.wav
 â”‚    ...
```

Each subfolder represents a **class label**.  
The model automatically encodes them as integer labels during training.

---

## ğŸ§® Training Details

| Parameter | Value |
|------------|--------|
| Model Type | Conditional WGAN-GP |
| Latent Dim | 100 |
| n_critic | 5 |
| Î»_gp | 10 |
| Learning Rate | 2e-4 |
| Optimizer | Adam (Î²â‚=0.5, Î²â‚‚=0.999) |
| Batch Size | 128 |
| n_mels | 80 |
| Frames per Sample | 512 |
| Sample Rate | 22050 Hz |

---

## ğŸš€ How to Run

### 1ï¸âƒ£ Install Dependencies

```bash
!pip install torch torchaudio torchvision tqdm matplotlib
```

### 2ï¸âƒ£ Set Dataset Path

Edit this line in the code:
```python
BASE_PATH = '/kaggle/input/the-frequency-quest/the-frequency-quest - Copy/train'
```

### 3ï¸âƒ£ Train the Model

```bash
python train_audio_gan.py
```

### 4ï¸âƒ£ During Training

Outputs are saved automatically:
```
gan_generated_audio/     â†’ Generated audio clips (.wav)
gan_spectrogram_plots/   â†’ Spectrograms each epoch
gan_loss_plot.png        â†’ Generator & Discriminator loss curves
```

---

## ğŸ¨ Visual & Audio Outputs

| Output Type | Description |
|--------------|-------------|
| ğŸ–¼ï¸ Spectrogram | Visualizes generator output over training |
| ğŸ§ Generated Audio | Converted `.wav` clips using HiFi-GAN |
| ğŸ“‰ Loss Curves | Shows training stability over epochs |

---

## ğŸ“ˆ Example Output Timeline

| Epoch | Quality | Description |
|--------|----------|-------------|
| 1 | ğŸŸ  Rough noise | Initial random audio |
| 50 | ğŸŸ¡ Semi-structured | Basic tonal patterns |
| 100+ | ğŸŸ¢ Realistic | Clear class-conditioned audio |

---

## ğŸ§  Core Concepts Used

- **Conditional GANs (CGAN)** â€” Learn label-conditioned generation  
- **WGAN-GP** â€” Uses Wasserstein distance + gradient penalty for stability  
- **HiFi-GAN Vocoder** â€” Converts Mel-spectrograms into realistic waveforms  
- **Mel-Spectrogram Representation** â€” Frequency vs time with perceptual scaling  

---

## ğŸ“‚ Repository Structure

```
â”œâ”€â”€ train_audio_gan.py          # Main training script
â”œâ”€â”€ gan_generated_audio/        # Generated samples
â”œâ”€â”€ gan_spectrogram_plots/      # Spectrogram visualizations
â”œâ”€â”€ gan_loss_plot.png           # Loss graph
â”œâ”€â”€ README.md                   # Project documentation
```

---

## ğŸ§‘â€ğŸ’» Authors & Contributors

| Name | Role | Institution |
|------|------|--------------|
| **Shiva Dubey** | Lead Developer | IIT Indore |
 â€” |

---

## ğŸ§¾ License

This project is released under the **MIT License**.  
You are free to use, modify, and distribute this for educational and research purposes.

---

## â­ Acknowledgements

- [HiFi-GAN: High-Fidelity Neural Vocoder](https://github.com/jik876/hifi-gan)  
- [Torchaudio](https://pytorch.org/audio/stable/index.html)  
- [WGAN-GP (Gulrajani et al., 2017)](https://arxiv.org/abs/1704.00028)  
- [Conditional GANs (Mirza & Osindero, 2014)](https://arxiv.org/abs/1411.1784)

---

<p align="center">
  <b>ğŸ§ The Frequency Quest â€” Redefining Audio Generation</b><br>
  <sub>Built with â¤ï¸ using PyTorch and HiFi-GAN</sub>
</p>
